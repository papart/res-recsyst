from __future__ import division

from sklearn.base import ClassifierMixin, BaseEstimator, clone, is_classifier
from sklearn.utils import array2d, check_random_state, check_arrays
from scipy.spatial import cKDTree, distance_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.grid_search import GridSearchCV
from sklearn.cross_validation import KFold
import numpy as np
import inspect
from abc import ABCMeta, abstractmethod
from sklearn.externals.six import with_metaclass
from sklearn.metrics.scorer import check_scoring
from sklearn.cross_validation import _check_cv


def _split_minor_major(X, y, sample_weight=None):
    """
    Returns values corresponding to major and minor classes (in two-class classification problem).
    """
    vals, idx = np.unique(y, return_inverse=True)
    minor_idx = np.array({True: idx}.get(
        np.sum(idx) < (len(idx) // 2), np.logical_not(idx)), dtype=bool)
    return X[minor_idx, :], y[minor_idx], X[np.logical_not(minor_idx),:], \
        y[np.logical_not(minor_idx)]


def Bootstrap(X, n_samples, random_state=None):
    rng = check_random_state(random_state)
    bootstrap_ind = rng.choice(len(X), size=(n_samples,), replace=True)
    return np.vstack((X, X[bootstrap_ind, :]))


def RUS(X, n_samples, random_state=None):
    """
    n_samples - number of samples to be removed
    """

    rng = check_random_state(random_state)
    rand_idx = rng.choice(len(X), len(X) - n_samples, replace=False)
    return X[rand_idx, :]


def SMOTE(X, n_samples, k=5, dist_power=2, multiple=False, sample_weight=None,
          random_state=None):
    """
    Returns n_samples of synthetic samples from X generated by SMOTE.

    Parameters
    ----------
    X : array-like, shape = [n_minority_samples, n_features]
        Holds the minority samples
    n_samples : int
        Number of new synthetic samples.
    k : int
        Number of nearest neighbours.
    dist_power : float, int
        Positive power in ditance metrics.
    random_state : None or int
        Seed for random generator.


    Returns
    -------
    smoted_X : array, shape = [n_samples, n_features]
        Synthetic samples
    """

    rng = check_random_state(random_state)
    n_minor, n_features = X.shape
    k = min([n_minor - 1, k])

    # Learn nearest neighbours
    nn_tree = cKDTree(X)

    if multiple:
        smoted_X = X.copy()
        if sample_weight is not None:
            weight_smoted = sample_weight.copy()
        nn_dist, nn_idx = nn_tree.query(smoted_X, k=k + 1, p=dist_power)
        nn_idx = nn_idx[:, 1:]
        for i in xrange(n_samples):
            start_idx = rng.choice(len(smoted_X))
            start = smoted_X[start_idx, :]
            end_idx = nn_idx[start_idx, rng.choice(k)]
            end = smoted_X[end_idx, :]
            shift = rng.rand()
            new_point = [start * shift + end * (1. - shift)]
            new_nn_idx = np.argsort(
                distance_matrix(smoted_X, new_point,
                                p=dist_power).T)[::-1][0][:k]
            smoted_X = np.vstack((smoted_X, new_point))
            nn_idx = np.vstack((nn_idx, new_nn_idx))
            if sample_weight is not None:
                weight_smoted = \
                    np.concatenate((weight_smoted,
                                    weight_smoted[start_idx] * shift +
                                    (1. - shift) * weight_smoted[end_idx]))

    else:
        start_indices = rng.choice(len(X), size=(n_samples,))
        starts = X[start_indices, :]
        nn_dists, nn_idx = nn_tree.query(starts, k=k + 1, p=dist_power)
        end_indices = nn_idx[np.arange(n_samples),
                             rng.choice(np.arange(1, k + 1), n_samples)]
        ends = X[end_indices, :]
        shifts = rng.rand(n_samples)
        smoted_X = starts * np.repeat(array2d(shifts).T, n_features, axis=1) \
            + ends * np.repeat(array2d(1. - shifts).T, n_features, axis=1)
        smoted_X = np.vstack((X, smoted_X))
        if sample_weight is not None:
            weight_smoted = sample_weight[start_indices] * shifts\
                + (1. - shifts) * sample_weight[end_indices]
    if sample_weight is None:
        return smoted_X
    else:
        return smoted_X, np.concatenate((sample_weight, weight_smoted))


def SINOP(X, n_samples, k=5, radius=None, pdf='uniform', mode='random',
          dist_power=2, sample_weight=None, random_state=None):
    """
    Returns n_samples of synthetic samples from X generated by SINOP.

    Parameters
    ----------
    X : array-like, shape = [n_minority_samples, n_features]
        Holds the minority samples
    n_samples : int
        Number of new synthetic samples.
    k : int
        Number of nearest neighbours.
    dist_power : float, int
        Positive power in ditance metrics.
    random_state : None or int
        Seed for random generator.


    Returns
    -------
    sinoped_X : array, shape = [n_samples, n_features]
        Synthetic samples
    """

    rng = check_random_state(random_state)
    n_minor, n_features = X.shape
    k = min([n_minor - 1, k])

    # Learn nearest neighbours
    nn_tree = cKDTree(X)
    nn_dists, nn_idx = nn_tree.query(X, k=k + 1, p=dist_power)
    mean_dists = np.mean(nn_dists, axis=1) * (k + 1) / k

    if radius is None:
        radius = np.mean(mean_dists)

    if mode == "random":
        proba = np.array([1.] * n_minor)
    elif mode == 'amplify':
        proba = 1 / mean_dists
    elif mode == 'align':
        proba = mean_dists
    else:
        raise ValueError("Wrong value of 'mode': %s", mode)

    # Normalizing probability to sum up to 1
    proba = proba / np.sum(proba)

    idx = rng.choice(n_minor, n_samples, replace=True, p=proba)

    centers = X[idx, :]

    if pdf in ['normal', 'uniform']:
        shifts = rng.multivariate_normal([0] * n_features,
                                         np.diag(
                                             [radius ** 2 / 4] * n_features),
                                         size=(n_samples,))
        if pdf == 'uniform':
            shifts /= np.linalg.norm(shifts, axis=1)[:, np.newaxis]
            shifts *= radius * rng.power(n_features,
                                         size=n_samples)[:, np.newaxis]
    else:
        raise ValueError("Wrong value of 'pdf': %s", pdf)

    X_sinoped = np.vstack((X, centers + shifts))

    if sample_weight is None:
        return X_sinoped
    else:
        return X_sinoped, np.concatenate((sample_weight, sample_weight[idx]))


class BaseResampleWrapper(with_metaclass(ABCMeta, BaseEstimator,
                                         ClassifierMixin)):

    """Base estimator wrapper for the imbalanced classification task.

    Notes
    -----
    This class should not be used directly. Use derived classes instead.

    """

    def __init__(self, base_estimator=None,
                 final_minor_ratio=None, estimator_params=tuple(),
                 random_state=None):
        self.base_estimator = base_estimator
        self.final_minor_ratio = final_minor_ratio
        self.estimator_params = estimator_params
        self.random_state = random_state
        self.default_estimator = DecisionTreeClassifier()

    def fit(self, X, y, sample_weight=None):
        X, y = check_arrays(X, y, copy=True, allow_lists=False)
        X, y = np.array(X), np.array(y)

        vals, idx = np.unique(y, return_inverse=True)
        minor_idx = np.array({True: idx}.get(
            np.sum(idx) < (len(idx) // 2), np.logical_not(idx)), dtype=bool)
        minor_X, minor_y = X[minor_idx, :], y[minor_idx]
        major_X, major_y = X[np.logical_not(minor_idx), :], \
            y[np.logical_not(minor_idx)]
        if sample_weight is not None:
            minor_weight = sample_weight[minor_idx]
            major_weight = sample_weight[np.logical_not(minor_idx)]
        else:
            minor_weight, major_weight = None, None

        self._validate_estimator(default=self.default_estimator,
                                 sample_weight=sample_weight)

        n_samples = self._define_minor_ratio(minor_X, minor_y,
                                             major_X, major_y)

        if n_samples > 0:
            X, y, sample_weight = \
                self._resample(minor_X, minor_y, major_X, major_y, n_samples,
                               minor_weight=minor_weight,
                               major_weight=major_weight)

        self._make_estimator()

        if sample_weight is not None:
            self.estimator_.fit(X, y, sample_weight=sample_weight)
        else:
            self.estimator_.fit(X, y)
        # if not hasattr(self.estimator_, 'classes_'):
        #     self.classes_ = self.estimator_.best_estimator_.classes_
        # else:
        self.classes_ = self.estimator_.classes_
        return self

    def _define_minor_ratio(self, minor_X, minor_y, major_X, major_y):
        self.initial_minor_ratio = len(minor_y) / len(major_y)

        if self.final_minor_ratio is None:
            self.final_minor_ratio = self.initial_minor_ratio
        if self.initial_minor_ratio > self.final_minor_ratio:
            # raise ValueError(
            #     'Inital minor class ratio is greater or equal to the desired.')
            return 0

        return int(np.floor((self.final_minor_ratio -
                             self.initial_minor_ratio) * len(major_y)))

    def _make_estimator(self):
        """Make and configure a copy of the `base_estimator_` attribute.

        Warning: This method should be used to properly instantiate new
        sub-estimators.
        """

        estimator = clone(self.base_estimator_)
        estimator.set_params(**dict((p, getattr(self, p))
                                    for p in self.estimator_params))

        self.estimator_ = estimator
        return estimator

    def predict(self, X):
        """Predict class for X.

        The predicted class of an input sample is computed as the predicted
        class of the underlying estimator.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape = [n_samples, n_features]
            The training input samples. Sparse matrices are accepted only if
            they are supported by the base estimator.

        Returns
        -------
        y : array of shape = [n_samples]
            The predicted classes.
        """
        X = check_arrays(X)
        return self.estimator_.predict(X)

    def predict_proba(self, X):
        """Predict class probabilities for X.

        The predicted class probabilities of an input sample is computed as
        the predicted class probabilities of the underlying estimators. If
        base estimators do not implement a ``predict_proba`` method, then
        ``NotImplementError`` is raise.

        Parameters
        ----------
        X : {array-like, sparse matrix} of shape = [n_samples, n_features]
            The training input samples. Sparse matrices are accepted only if
            they are supported by the base estimator.

        Returns
        -------
        proba : array of shape = [n_samples, n_classes]
            The class probabilities of the input samples. The order of the
            classes corresponds to that in the attribute `classes_`.
        """
        # Check data
        X, = check_arrays(X)

        if not hasattr(self.estimator_, "predict_proba"):
            raise NotImplementedError(
                'Underlying estimator of class' + self.estimator_.__class__ +
                'has no attibute ``pedict_proba``.')

        return self.estimator_.predict_proba(X)

    # @abstractmethod
    def _resample(self, minor_X, minor_y, major_X, major_y, n_samples,
                  minor_weight=None, major_weight=None):
        if minor_weight is None:
            return np.vstack((minor_X, major_X)), \
                np.concatenate((minor_y, major_y)), None
        else:
            return np.vstack((minor_X, major_X)), \
                np.concatenate((minor_y, major_y)), \
                np.concatenate((minor_weight, major_weight))

    def _validate_estimator(self, default=None, sample_weight=None):
        """Check the estimator , set the `base_estimator_` attribute."""
        if self.base_estimator is not None:
            self.base_estimator_ = self.base_estimator
        else:
            self.base_estimator_ = default

        if self.base_estimator_ is None:
            raise ValueError("base_estimator cannot be None")

        if sample_weight is not None and \
            not ('sample_weight' in
                 inspect.getargspec(self.base_estimator_.fit)[0]):
            raise NotImplementedError(
                "Underlying classifier " + self.base_estimator.__class__ +
                " does not support sample weights.")


class AutomaticResamplerWrapper(with_metaclass(ABCMeta, BaseResampleWrapper)):

    """Base estimator wrapper with automatic selection of minor/major ratio
        for the imbalanced classification task.

    Notes
    -----
    This class should not be used directly. Use derived classes instead.

    """

    def __init__(self, random_state=None):
        self.random_state = random_state


class BootstrapWrapper(BaseResampleWrapper):

    def _resample(self, minor_X, minor_y, major_X, major_y, n_samples,
                  minor_weight=None, major_weight=None):

        rng = check_random_state(self.random_state)
        bootstrap_ind = rng.choice(
            len(minor_y), size=(n_samples,), replace=True)

        minor_y = np.concatenate((minor_y, minor_y[bootstrap_ind]))
        minor_X = np.vstack((minor_X, minor_X[bootstrap_ind, :]))
        if minor_weight is not None:
            sample_weight = np.concatenate(
                (minor_weight, minor_weight[bootstrap_ind], major_weight))
        else:
            sample_weight = None

        return np.vstack((minor_X, major_X)),\
            np.concatenate((minor_y, major_y)), sample_weight


class SMOTEWrapper(BaseResampleWrapper):

    def __init__(self, base_estimator=None,
                 final_minor_ratio=1.0, estimator_params=tuple(),
                 random_state=None,  k=5, dist_power=2, multiple=False):
        super(SMOTEWrapper, self).__init__(base_estimator=base_estimator,
                                           final_minor_ratio=final_minor_ratio,
                                           estimator_params=estimator_params,
                                           random_state=random_state)
        self.k = k
        self.dist_power = dist_power
        self.multiple = multiple

    def _resample(self, minor_X, minor_y, major_X, major_y, n_samples,
                  minor_weight=None, major_weight=None):
        if minor_weight is not None:
            smoted_X, minor_weight = SMOTE(minor_X, n_samples, k=self.k,
                                           multiple=self.multiple,
                                           dist_power=self.dist_power,
                                           random_state=self.random_state,
                                           sample_weight=minor_weight)
            sample_weight = np.concatenate((minor_weight, major_weight))
        else:
            smoted_X = SMOTE(minor_X, n_samples, k=self.k,
                             multiple=self.multiple,
                             dist_power=self.dist_power,
                             random_state=self.random_state)
            sample_weight = None

        return np.vstack((smoted_X, major_X)),\
            np.concatenate((minor_y, [minor_y[0]] * n_samples, major_y)),\
            sample_weight


class SINOPWrapper(BaseResampleWrapper):

    def __init__(self, base_estimator=None,
                 final_minor_ratio=1.0, estimator_params=tuple(),
                 random_state=None, k=5, dist_power=2,
                 mode='random', pdf='uniform',
                 radius=None):
        super(SINOPWrapper, self).__init__(base_estimator=base_estimator,
                                           final_minor_ratio=final_minor_ratio,
                                           estimator_params=estimator_params,
                                           random_state=random_state)
        self.k = k
        self.dist_power = dist_power
        self.mode = mode
        self.pdf = pdf
        self.radius = radius

    def _resample(self, minor_X, minor_y, major_X, major_y, n_samples,
                  minor_weight=None, major_weight=None):
        if minor_weight is not None:
            sinoped_X, minor_weight = \
                SINOP(minor_X, n_samples, k=self.k, pdf=self.pdf,
                      mode=self.mode, dist_power=self.dist_power,
                      radius=self.radius, random_state=self.random_state,
                      sample_weight=minor_weight)
            sample_weight = np.concatenate((minor_weight, major_weight))
        else:
            sinoped_X = SINOP(minor_X, n_samples, k=self.k, pdf=self.pdf,
                              mode=self.mode, dist_power=self.dist_power,
                              radius=self.radius,
                              random_state=self.random_state)
            sample_weight = None
        return np.vstack((sinoped_X, major_X)),\
            np.concatenate((minor_y, [minor_y[0]] * n_samples, major_y)),\
            sample_weight


class RandomUndersamplingWrapper(BaseResampleWrapper):

    """Wrapper for random undersampling strategy."""

    def _resample(self, minor_X, minor_y, major_X, major_y, n_samples,
                  major_weight=None, minor_weight=None):
        rng = check_random_state(self.random_state)
        rand_idx = rng.choice(
            len(major_y), len(major_y) - n_samples, replace=False)
        if minor_weight is not None:
            sample_weight = np.concatenate((minor_weight,
                                            major_weight[rand_idx]))
        else:
            sample_weight = None

        return np.vstack((minor_X, major_X[rand_idx, :])),\
            np.concatenate((minor_y, major_y[rand_idx])), sample_weight


class UnifiedResamplingWrapper(BaseResampleWrapper):

    def __init__(self,
                 base_estimator=None,
                 wrappers=(RandomUndersamplingWrapper(), SMOTEWrapper()),
                 wrappers_params=None, minor_ratios=None):
        if wrappers_params is None:
            wrappers_params = []
            for i in xrange(len(wrappers)):
                wrappers_params.append({})
        if minor_ratios is None:
            minor_ratios = [0.5, 1.0]
        if len(wrappers) != len(minor_ratios):
            raise ValueError("Length of ratios is %i, and number of" +
                             "wrappers is %i"
                             % (len(minor_ratios), len(wrappers)))
        if len(wrappers) != len(wrappers_params):
            raise ValueError("Length of parameters is %i, and number of" +
                             "wrappers is %i"
                             % (len(wrappers_params), len(wrappers)))
        self.wrappers = wrappers
        self.minor_ratios = minor_ratios

        if any([not isinstance(val, BaseResampleWrapper) for val in wrappers]):
            raise "Internal wrappers could be only of" +\
                "BaseResampleWrapper class."

        wrappers = wrappers[::-1]
        wrappers_params = wrappers_params[::-1]
        minor_ratios = minor_ratios[::-1]

        internal = clone(wrappers[0])
        internal.set_params(base_estimator=base_estimator,
                            final_minor_ratio=minor_ratios[0])

        internal.set_params(**dict((p, getattr(self, p))
                                   for p in wrappers_params[0]))
        for est, ratio, params in zip(wrappers[1:], minor_ratios[1:],
                                      wrappers_params[1:]):
            est = clone(est)
            est.set_params(base_estimator=internal, final_minor_ratio=ratio)
            internal = est
            internal.set_params(**dict((p, getattr(self, p))
                                       for p in params))
        self.estimator_ = internal

    def fit(self, X, y, sample_weight=None):
        self.estimator_.fit(X, y, sample_weight)
        self.classes_ = self.estimator_.classes_
        return self


# class ResamplingWrapperCV(AutomaticResamplerWrapper):

#     def __init__(self, base_wrapper=None, wrapper_params={}, scoring=None,
#                  cv=None, minor_ratios=None, random_state=None,
#                  n_jobs=1, pre_dispatch='2*n_jobs'):
#         self.base_estimator = base_wrapper
#         self.estimator_params = wrapper_params
#         self.scoring = scorer = check_scoring(base_wrapper, scoring=scoring)
#         self.cv = {True: 3}.get(cv is None, cv)
#         self.minor_ratios = minor_ratios
#         self.random_state = random_state
#         self.n_jobs = n_jobs
#         self.pre_dispatch = pre_dispatch
#         self.default_estimator = SMOTEWrapper()

#     def _validate_estimator(self, default, sample_weight=None):
#         super(ResamplingWrapperCV,
#               self)._validate_estimator(default, sample_weight=None)
#         if not isinstance(self.base_estimator_, BaseResampleWrapper):
#             raise "Wrapper could be only of BaseResampleWrapper class."

#     def _define_minor_ratio(self, minor_X, minor_y, major_X, major_y):
#         self.initial_minor_ratio = len(minor_y) / len(major_y)
#         if self.minor_ratios is None:
#             self.minor_ratios = np.linspace(
#                 self.initial_minor_ratio + 1 / len(major_y), 1.0, 50)

#     def _resample(self, minor_X, minor_y, major_X, major_y, n_samples,
#                   major_weight=None, minor_weight=None):
#         cv_scores = []
#         for ratio in self.minor_ratios:
#             score = 0
#             for major_train, major_test in KFold(
#                     len(major_y), self.cv, random_state=self.random_state, shuffle=True):
#                 for minor_train, minor_test in KFold(
#                         len(minor_y), self.cv, random_state=self.random_state, shuffle=True):
#                     estimator = self._make_estimator(ratio)
#                     X = np.vstack(
#                         (minor_X[minor_train, :], major_X[major_train]))
#                     y = np.concatenate(
#                         (minor_y[minor_train], major_y[major_train]))
#                     estimator.fit(X, y)
#                     X = np.vstack(
#                         (minor_X[minor_test, :], major_X[major_test]))
#                     y = np.concatenate(
#                         (minor_y[minor_test], major_y[major_test]))
#                     score += self.scoring(estimator, X, y)
#             cv_scores.append(score)
#         self.final_minor_ratio = self.minor_ratios[np.argmax(cv_scores)]
#         self.estimator_ = self._make_estimator(self.final_minor_ratio)
#         return super(ResamplingWrapperCV,
#                      self)._resample(minor_X, minor_y, major_X, major_y,
#                                      n_samples, major_weight, minor_weight)

#     def _make_estimator(self, minor_ratio=None):
#         if minor_ratio is not None:
#             estimator = clone(self.base_estimator_)
#             estimator.set_params(**dict((p, getattr(self, p))
#                                         for p in self.estimator_params))
#             estimator.set_params(final_minor_ratio=minor_ratio)
#             self.estimator_ = estimator
#             return estimator

class ResamplingWrapperCV(BaseResampleWrapper):

    def __init__(self, base_wrapper=None, wrapper_params=None, scoring=None,
                 cv=None, minor_ratios=None, random_state=None,
                 n_jobs=1, pre_dispatch='2*n_jobs'):
        self.base_estimator = base_wrapper
        self.estimator_params = {True: {}}.get(
            wrapper_params is None, wrapper_params)
        self.scoring = check_scoring(base_wrapper, scoring=scoring)
        self.cv = {True: 3}.get(cv is None, cv)
        self.minor_ratios = minor_ratios
        self.random_state = random_state
        self.n_jobs = n_jobs
        self.pre_dispatch = pre_dispatch
        self.default_estimator = SMOTEWrapper()
        self.cv_scores = []

    def _validate_estimator(self, default, sample_weight=None):
        super(ResamplingWrapperCV,
              self)._validate_estimator(default, sample_weight=None)
        if not isinstance(self.base_estimator_, BaseResampleWrapper):
            raise "Wrapper could be only of BaseResampleWrapper class."

    def _define_minor_ratio(self, minor_X, minor_y, major_X, major_y):
        self.initial_minor_ratio = len(minor_y) / len(major_y)
        if self.minor_ratios is None:
            self.minor_ratios = np.linspace(
                self.initial_minor_ratio + 1 / len(major_y), 1.0, 10)
        return 1

    def _resample(self, minor_X, minor_y, major_X, major_y, n_samples,
                  major_weight=None, minor_weight=None):
        for ratio in self.minor_ratios:
            scores = []
            for major_train, major_test in KFold(
                    len(major_y), self.cv, random_state=self.random_state, shuffle=True):
                for minor_train, minor_test in KFold(
                        len(minor_y), self.cv, random_state=self.random_state, shuffle=True):
                    estimator = self._make_estimator(ratio)
                    X = np.vstack(
                        (minor_X[minor_train, :], major_X[major_train]))
                    y = np.concatenate(
                        (minor_y[minor_train], major_y[major_train]))
                    estimator.fit(X, y)
                    X = np.vstack(
                        (minor_X[minor_test, :], major_X[major_test]))
                    y = np.concatenate(
                        (minor_y[minor_test], major_y[major_test]))
                    scores.append(self.scoring(estimator, X, y))
            self.cv_scores.append(np.mean(scores))
        self.final_minor_ratio = self.minor_ratios[np.argmax(self.cv_scores)]
        #self.estimator_ = self._make_estimator(self.final_minor_ratio)
        self.estimator_params['final_minor_ratio'] = self.final_minor_ratio
        return super(ResamplingWrapperCV,
                     self)._resample(minor_X, minor_y, major_X, major_y,
                                     n_samples, major_weight, minor_weight)

    def _make_estimator(self, minor_ratio=None):
        if minor_ratio is not None:
            estimator = clone(self.base_estimator_)
            estimator.set_params(**dict((p, getattr(self, p))
                                        for p in self.estimator_params))
            estimator.set_params(final_minor_ratio=minor_ratio)
            self.estimator_ = estimator
            return estimator
        else:
            BaseResampleWrapper._make_estimator(self)


if __name__ == '__main__':
    from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier
    from sklearn.metrics import average_precision_score

    # np.random.seed(0)

    # est = SINOPWrapper(final_minor_ratio=0.5, mode='amplify', pdf='uniform')
    # est = SMOTEWrapper(final_minor_ratio=0.5, multiple=False)
    # est = RandomUndersamplingWrapper(final_minor_ratio=0.5)
    # est = BootstrapWrapper(final_minor_ratio=0.5)
    # est = UnifiedResamplingWrapper(minor_ratios=[0.5, 1.0])
    est = ResamplingWrapperCV(
        base_wrapper=SINOPWrapper(), scoring='roc_auc', cv=3)

    est = AdaBoostClassifier(base_estimator=est)
    # est = BaggingClassifier(base_estimator=est)

    train_X = np.random.rand(100, 2)
    train_y = np.sum(train_X, 1) > 1.75
    a, b, c, d = _split_minor_major(train_X, train_y)
    # print sum(b), len(b), sum(d), len(d)
    test_X = np.random.rand(1000, 2)
    est.fit(train_X, train_y)
    print est.cv_scores
    # print est.predict(test_X)
    # print LogisticRegression().fit(train_X, train_y).predict(test_X)
    print 'AUC:', average_precision_score(np.sum(test_X, 1) > 1.5, est.predict_proba(test_X)[:, 1])
    # print est.final_minor_ratio
